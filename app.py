import os
import streamlit as st
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer

# --- ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ê¸°ëŠ¥ ì„í¬íŠ¸ ---
from pdf_to_md import main as process_all_pdfs
from setup_chroma import setup_vector_store

# --- Langchain ê´€ë ¨ ì„í¬íŠ¸ ---
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_google_genai import GoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.schema.output_parser import StrOutputParser

# --- 1. ì„¤ì • (Configuration) ---
load_dotenv()

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
CHROMA_DB_PATH = "chroma_db"
EMBEDDING_MODEL_PATH = "./ko-sroberta-multitask"  # ë¡œì»¬ ê²½ë¡œ
REMOTE_EMBEDDING_MODEL = "jhgan/ko-sroberta-multitask" # Hugging Face ëª¨ë¸
LLM_MODEL_NAME = "gemini-2.5-pro"
SOURCE_DOCS_DIR = "source_documents"

# --- 2. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ (Core Functions) ---

@st.cache_resource
def load_retriever():
    """
    Chroma DBì—ì„œ Retrieverë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    """
    # 1. ë¡œì»¬ ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìë™ ë‹¤ìš´ë¡œë“œ
    if not os.path.exists(EMBEDDING_MODEL_PATH):
        with st.spinner(f"'{REMOTE_EMBEDDING_MODEL}' ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤... (ìµœì´ˆ 1íšŒ)"): 
            try:
                model = SentenceTransformer(REMOTE_EMBEDDING_MODEL)
                model.save(EMBEDDING_MODEL_PATH)
            except Exception as e:
                st.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.stop()

    # 2. DB ë° Retriever ë¡œë“œ
    if not os.path.exists(CHROMA_DB_PATH):
        st.warning("ì§€ì‹ ë² ì´ìŠ¤(Chroma DB)ê°€ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.")
        return None
    try:
        embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_PATH)
        vector_store = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
        return vector_store.as_retriever(search_kwargs={'k': 5})
    except Exception as e:
        st.error(f"Retriever ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

@st.cache_resource
def load_llm():
    """Google Generative AI ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not GEMINI_API_KEY:
        st.error("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
    try:
        return GoogleGenerativeAI(model=LLM_MODEL_NAME, temperature=0.7, google_api_key=GEMINI_API_KEY)
    except Exception as e:
        st.error(f"LLM ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

def create_rag_chain(_retriever, _llm):
    """RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    prompt_template = """
    ë‹¹ì‹ ì€ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    
    # ì»¨í…ìŠ¤íŠ¸:
    {context}
    
    # ì§ˆë¬¸: {question}
    
    # ë‹µë³€:
    """
    prompt = PromptTemplate.from_template(prompt_template)

    def format_docs(docs):
        return "\n\n".join(f"[ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}]\n{doc.page_content}" for doc in docs)

    return (
        {"context": _retriever | RunnableLambda(format_docs), "question": RunnablePassthrough()}
        | prompt
        | _llm
        | StrOutputParser()
    )

# --- 3. Streamlit UI ë° ë©”ì¸ ë¡œì§ (UI & Main Logic) ---

st.set_page_config(page_title="ë‚˜ë§Œì˜ RAG ì§€ì‹ ì„¼í„°", page_icon="ğŸ§ ")
st.title("ğŸ§  ë‚˜ë§Œì˜ RAG ì§€ì‹ ì„¼í„°")
st.write("PDFë¥¼ ì—…ë¡œë“œí•˜ì—¬ ë‚˜ë§Œì˜ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ë§Œë“¤ê³ , ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")

# --- ì‚¬ì´ë“œë°”: íŒŒì¼ ì—…ë¡œë“œ ë° DB ê´€ë¦¬ ---
with st.sidebar:
    st.header("ì§€ì‹ ë² ì´ìŠ¤ ê´€ë¦¬")
    
    uploaded_files = st.file_uploader(
        "ì—¬ê¸°ì— PDF íŒŒì¼ì„ ë“œë˜ê·¸ ì•¤ ë“œë¡­í•˜ì„¸ìš”",
        type="pdf",
        accept_multiple_files=True
    )

    if st.button("ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸"):
        if uploaded_files:
            if not os.path.exists(SOURCE_DOCS_DIR):
                os.makedirs(SOURCE_DOCS_DIR)

            for file in uploaded_files:
                file_path = os.path.join(SOURCE_DOCS_DIR, file.name)
                with open(file_path, "wb") as f:
                    f.write(file.getbuffer())
            st.success(f"{len(uploaded_files)}ê°œì˜ PDF íŒŒì¼ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

            with st.spinner("PDFë¥¼ ì²˜ë¦¬í•˜ì—¬ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                process_all_pdfs()
                setup_vector_store()
                st.cache_resource.clear()
            st.success("ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
            st.rerun()
        else:
            st.warning("ì—…ë°ì´íŠ¸í•  PDF íŒŒì¼ì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.")

# --- ë©”ì¸ í™”ë©´: QA ì±—ë´‡ ---

retriever = load_retriever()
llm = load_llm()

if retriever and llm:
    rag_chain = create_rag_chain(retriever, llm)

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if user_prompt := st.chat_input("ì§€ì‹ ë² ì´ìŠ¤ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": user_prompt})
        with st.chat_message("user"):
            st.markdown(user_prompt)

        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                response = rag_chain.invoke(user_prompt)
                st.markdown(response)
        
        st.session_state.messages.append({"role": "assistant", "content": response})
else:
    st.info("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'ì§€ì‹ ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    